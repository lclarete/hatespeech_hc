{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b0954a1a",
   "metadata": {
    "id": "b0954a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import all necessary libraries\n",
    "#supervised and unsupervised (not part of proposal) classification. FastText is used for both. For the flat, just using traditional ML models. \n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# testing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# pre-processing\n",
    "import re\n",
    "import string \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "nltk.download('punkt')\n",
    "punctuation = list(set(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3585a9fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "3585a9fa",
    "outputId": "704ed926-5ce7-4a31-b60b-fba34fce98c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 80)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Hate.speech</th>\n",
       "      <th>Sexism</th>\n",
       "      <th>Body</th>\n",
       "      <th>Racism</th>\n",
       "      <th>Ideology</th>\n",
       "      <th>Homophobia</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Health</th>\n",
       "      <th>...</th>\n",
       "      <th>Thin.women</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>East.europeans</th>\n",
       "      <th>Africans</th>\n",
       "      <th>South.Americans</th>\n",
       "      <th>Brazilians</th>\n",
       "      <th>Migrants</th>\n",
       "      <th>Homossexuals</th>\n",
       "      <th>Thin.people</th>\n",
       "      <th>Ageing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"não come mel, morde marimbondo\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>não tem pinto, tem orgulho !</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  Hate.speech  Sexism  Body  Racism  \\\n",
       "0  \"não come mel, morde marimbondo\"            0       0     0       0   \n",
       "1      não tem pinto, tem orgulho !            0       0     0       0   \n",
       "\n",
       "   Ideology  Homophobia  Origin  Religion  Health  ...  Thin.women  Arabic  \\\n",
       "0         0           0       0         0       0  ...           0       0   \n",
       "1         0           0       0         0       0  ...           0       0   \n",
       "\n",
       "   East.europeans  Africans  South.Americans  Brazilians  Migrants  \\\n",
       "0               0         0                0           0         0   \n",
       "1               0         0                0           0         0   \n",
       "\n",
       "   Homossexuals  Thin.people  Ageing  \n",
       "0             0            0       0  \n",
       "1             0            0       0  \n",
       "\n",
       "[2 rows x 80 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get hierarchical_data\n",
    "\n",
    "hierarchical_data = pd.read_csv('https://raw.githubusercontent.com/paulafortuna/Portuguese-Hate-Speech-Dataset/master/2019-05-28_portuguese_hate_speech_hierarchical_classification.csv')\n",
    "print(hierarchical_data.shape)\n",
    "hierarchical_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "MV-t4dzWK-u_",
   "metadata": {
    "id": "MV-t4dzWK-u_"
   },
   "outputs": [],
   "source": [
    "# ! pip install -U spacy\n",
    "# ! python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "IGNcbsMvLvWU",
   "metadata": {
    "id": "IGNcbsMvLvWU"
   },
   "outputs": [],
   "source": [
    "# ! pip install pt_core_news_lg\n",
    "# import pt_core_news_lg\n",
    "# nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "06366f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_manual = ['http?', 'não' , 'mais', 'is?o', 'es[st]?*', 'quan[dt]?', ' ', '\\n', '...', 'de o', 'em o', 'rt', 'ter', 'pra', 'a o', 'q', '  ', '..', 'por 0', 'fazer', 'dizer', 'vc']\n",
    "\n",
    "def preprocessing(text):\n",
    "    l = []\n",
    "    split_sentence = text.split()\n",
    "    for word in split_sentence:\n",
    "        if len(word) > 2 and word not in stopwords and word not in stopwords_manual and word not in punctuation:\n",
    "            word = word.lower()\n",
    "            word = re.sub('@[\\w]+','',word) #remove usernames\n",
    "            word = re.sub('\"','',word) #remove quotes\n",
    "            word = re.sub(',','',word) #remove comma\n",
    "            word = re.sub('!','',word) #remove dote\n",
    "            word = re.sub('\\.','',word) #remove dote\n",
    "            word = re.sub('-',' ',word) #remove dote\n",
    "            word = re.sub(';',' ',word) #remove dote\n",
    "            word = re.sub('\\?',' ',word) #remove dote\n",
    "            word = re.sub('/',' ',word).strip() #remove dote\n",
    "            l.append(word)\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "9b39aa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            come mel morde marimbondo\n",
       "1                                        pinto orgulho\n",
       "2          merda crepúsculo cinebiografia chuck norris\n",
       "3                      tapa bundinha cotovelada costas\n",
       "4    diminutivo inho acompanha trajetória homem ver...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchical_data['text'] = hierarchical_data.text.apply(preprocessing)\n",
    "hierarchical_data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8183f225",
   "metadata": {
    "id": "8183f225"
   },
   "outputs": [],
   "source": [
    "# selecting X and y variables\n",
    "X = hierarchical_data.text\n",
    "y = hierarchical_data[\"Hate.speech\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "d20048a6",
   "metadata": {
    "id": "d20048a6"
   },
   "outputs": [],
   "source": [
    "# 10-fold crossvalidation (Chollet, 2017), combined with holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "0bd93ce9",
   "metadata": {
    "id": "0bd93ce9"
   },
   "outputs": [],
   "source": [
    "def model_evaluate(X, y, model, model_name, n_splits:int=10):\n",
    "    # kfold with 10 splits split\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # set a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # metrics\n",
    "    scoring = ['accuracy', 'f1', 'f1_micro', 'f1_macro', 'precision', 'recall']\n",
    "\n",
    "    # cross validation\n",
    "    scores = cross_validate(pipeline, X, y, scoring=scoring, cv=kf)\n",
    "    \n",
    "    # format the results into a dataframe\n",
    "    results = pd.DataFrame(scores)\n",
    "    results['model_name'] = model_name\n",
    "    # save it into the results directory\n",
    "#     results.to_csv(f'data/results/{model_name}.csv')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "9bd3fb0b",
   "metadata": {
    "id": "9bd3fb0b"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'ComplementNB':ComplementNB(),\n",
    "    'LogisticRegression':LogisticRegression(),\n",
    "    'ForestClassifier':RandomForestClassifier(max_depth=4, n_estimators=150),\n",
    "    'xgboost': xgb.XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "4d5866dc",
   "metadata": {
    "id": "4d5866dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "for k, v in models.items():\n",
    "    results = model_evaluate(X, y, v, k)\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "2248f5ed",
   "metadata": {
    "id": "2248f5ed"
   },
   "outputs": [],
   "source": [
    "total_results = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "k3Q3-orP13SI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "k3Q3-orP13SI",
    "outputId": "ee859989-447f-41f3-ed03-db87068a6698"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_f1_micro</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ComplementNB</th>\n",
       "      <td>0.054601</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>0.845978</td>\n",
       "      <td>0.614110</td>\n",
       "      <td>0.845978</td>\n",
       "      <td>0.758942</td>\n",
       "      <td>0.670826</td>\n",
       "      <td>0.566780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ForestClassifier</th>\n",
       "      <td>0.243105</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.783345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.783345</td>\n",
       "      <td>0.439256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.200961</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.868736</td>\n",
       "      <td>0.602591</td>\n",
       "      <td>0.868736</td>\n",
       "      <td>0.761986</td>\n",
       "      <td>0.873751</td>\n",
       "      <td>0.460096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.579769</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>0.870147</td>\n",
       "      <td>0.638321</td>\n",
       "      <td>0.870147</td>\n",
       "      <td>0.779588</td>\n",
       "      <td>0.803925</td>\n",
       "      <td>0.530108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    fit_time  score_time  test_accuracy   test_f1  \\\n",
       "model_name                                                          \n",
       "ComplementNB        0.054601    0.006865       0.845978  0.614110   \n",
       "ForestClassifier    0.243105    0.017436       0.783345  0.000000   \n",
       "LogisticRegression  0.200961    0.007065       0.868736  0.602591   \n",
       "xgboost             0.579769    0.015465       0.870147  0.638321   \n",
       "\n",
       "                    test_f1_micro  test_f1_macro  test_precision  test_recall  \n",
       "model_name                                                                     \n",
       "ComplementNB             0.845978       0.758942        0.670826     0.566780  \n",
       "ForestClassifier         0.783345       0.439256        0.000000     0.000000  \n",
       "LogisticRegression       0.868736       0.761986        0.873751     0.460096  \n",
       "xgboost                  0.870147       0.779588        0.803925     0.530108  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_results.groupby('model_name').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc89b71",
   "metadata": {
    "id": "4cc89b71"
   },
   "outputs": [],
   "source": [
    "# Next steps\n",
    "# # grid search\n",
    "\n",
    "# # https://www.kaggle.com/code/carlosdg/xgboost-with-scikit-learn-pipeline-gridsearchcv/notebook\n",
    "# model = xgb.XGBClassifier()\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('standard_scaler', StandardScaler()), \n",
    "#     ('pca', PCA()), \n",
    "#     ('model', model)\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'pca__n_components': [5, 10, 15, 20, 25, 30],\n",
    "#     'model__max_depth': [2, 3, 5, 7, 10],\n",
    "#     'model__n_estimators': [10, 100, 500],\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "# grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20015d16",
   "metadata": {
    "id": "20015d16"
   },
   "outputs": [],
   "source": [
    "# xgBoost input\n",
    "# and we save the last layer before the classification to extract 50 dimensions as input to the xgBoost algorithm,7 which is a gradient boosting implementation from the Python library (Chen and Guestrin, 2016).\n",
    "# or xgBoost, the default parameter setting has been used, except for ‘eta’ and ‘gamma’. In this case, we conducted a grid search combining several values of both (eta: 0, 0.3, 1; and gamma: 0.1,1, 10) in order to obtain the optimal eta and gamma settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154ab21",
   "metadata": {
    "id": "2154ab21"
   },
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "# Methods provided by Keras are then used to map each token in the input to an embedding.\n",
    "# feature extraction: glove 300 dimentions\n",
    "# pre-trained Glove word embeddings with 300 dimensions for Portuguese (Hartmann et al., 2017). \n",
    "\n",
    "# classification: lstm\n",
    "# https://github.com/paulafortuna/twitter-hatespeech/blob/master/lstm.py\n",
    "\n",
    "# The architecture contains an embedding Layer with the weights from the word embeddings \n",
    "# extraction procedure, an additional LSTM layer with 50 dimensions, and dropouts at the end of both layers.\n",
    "\n",
    "# As loss function, we used binary crossentropy and for optimization Adam, 10 epochs and 128 for batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VNtJ1vrH3ewA",
   "metadata": {
    "id": "VNtJ1vrH3ewA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
