{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914b7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! export MACOSX_DEPLOYMENT_TARGET=10.9\n",
    "# ! pip install extremetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3af135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94adf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mwydmuch/extremeText\n",
    "# https://arxiv.org/pdf/1810.11671v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "42576553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import extremeText\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "pt_stopwords = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "905b7780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 80)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Hate.speech</th>\n",
       "      <th>Sexism</th>\n",
       "      <th>Body</th>\n",
       "      <th>Racism</th>\n",
       "      <th>Ideology</th>\n",
       "      <th>Homophobia</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Health</th>\n",
       "      <th>...</th>\n",
       "      <th>Thin.women</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>East.europeans</th>\n",
       "      <th>Africans</th>\n",
       "      <th>South.Americans</th>\n",
       "      <th>Brazilians</th>\n",
       "      <th>Migrants</th>\n",
       "      <th>Homossexuals</th>\n",
       "      <th>Thin.people</th>\n",
       "      <th>Ageing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"não come mel, morde marimbondo\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>não tem pinto, tem orgulho !</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  Hate.speech  Sexism  Body  Racism  \\\n",
       "0  \"não come mel, morde marimbondo\"            0       0     0       0   \n",
       "1      não tem pinto, tem orgulho !            0       0     0       0   \n",
       "\n",
       "   Ideology  Homophobia  Origin  Religion  Health  ...  Thin.women  Arabic  \\\n",
       "0         0           0       0         0       0  ...           0       0   \n",
       "1         0           0       0         0       0  ...           0       0   \n",
       "\n",
       "   East.europeans  Africans  South.Americans  Brazilians  Migrants  \\\n",
       "0               0         0                0           0         0   \n",
       "1               0         0                0           0         0   \n",
       "\n",
       "   Homossexuals  Thin.people  Ageing  \n",
       "0             0            0       0  \n",
       "1             0            0       0  \n",
       "\n",
       "[2 rows x 80 columns]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/2019-05-28_portuguese_hate_speech_hierarchical_classification.csv')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d52e3",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9622f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_manual = ['http?', 'mais', 'is?o', 'es[st]?*', 'quan[dt]?', ' ', '\\n', '...', 'de o', 'em o', 'rt', 'ter', 'pra', 'a o', 'q', '  ', '..', 'por 0', 'fazer', 'dizer', 'vc']\n",
    "\n",
    "def preprocessing(text):\n",
    "    l = []\n",
    "#     text = ' '.join([w.lemma_ for w in nlp(text)]) #This lemma also performs tokenization)\n",
    "    split_sentence = text.split()\n",
    "    for word in split_sentence:\n",
    "        if len(word) > 3 and word not in pt_stopwords and word not in stopwords_manual: # and word not in punctuation:\n",
    "            word = word.lower()\n",
    "            word = re.sub('@[\\w]+','',word) #remove usernames\n",
    "            word = re.sub('\"','',word) #remove quotes\n",
    "            word = re.sub(',','',word) #remove puntuation\n",
    "            word = re.sub('!','',word) #remove puntuation\n",
    "            word = re.sub('\\.','',word) #remove puntuation\n",
    "            word = re.sub('-',' ',word) #remove puntuation\n",
    "            word = re.sub(';',' ',word) #remove puntuation\n",
    "            word = re.sub('\\?',' ',word) #remove puntuation\n",
    "            word = re.sub('/',' ',word).strip() #remove puntuation\n",
    "\n",
    "            l.append(word)\n",
    "    return ' '.join(l)\n",
    "\n",
    "# def preprocessing(text):\n",
    "\n",
    "#     text = re.sub('@[\\w]+','',text) #remove usernames\n",
    "#     text = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', text) #remove links\n",
    "#     text = [w.lemma_ for w in nlp(text)] #This lemma also performs tokenization\n",
    "#     text = [word for word in text if word not in punctuation]\n",
    "#     text = [w.lower() for w in text]\n",
    "#     text = [word for word in text if word not in stopwords and word not in stopwords_manual]\n",
    "#     text = ' '.join([str(word) for word in text])\n",
    "\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6e10a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.text.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "23cdd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data should follow this format from extreme text\n",
    "\n",
    "train_data_format = \"\"\"\n",
    "__label__mariadb-galera __label__mariadb55-mariadb __label__mysql55-mysql mariadb mariadb mysql solaris vulnerability oracle mysql server users availability vectors keys oracle com technetwork topics security html http secunia com http www oracle com technetwork topics security http lists security announce msg00016 html http www oracle com technetwork topics security html http secunia com http www securityfocus security gentoo glsa xml mariadb-galera mariadb55-mariadb-devel ruby-mysql openshift-origin-cartridge-mysql rh-mariadb100-mariadb mariadb-apb-role query-mysql mariadb55-mariadb-test rh-mysql57-mysql rh-mariadb101-mariadb rh-mysql56-mysql mysql mysql-connector-java mariadb55-mariadb-bench mysql55-mysql mysql-apb-role mysql mariadb55-mariadb-server mysql-binuuid-rails rh-mysql80-mysql com.github.brandtg switchboard-mysql rh-mariadb102-mariadb mariadb mariadb55-mariadb rhn-solaris-bootstrap mariadb55-mariadb-libs\n",
    "\"\"\"\n",
    "# https://github.com/automated-library/ICPC_2022_Automated-Identification-of-Libraries-from-Vulnerability-Data/tree/main/extremeText/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f1d1fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3k/n8m_c1vj5gb6gyfmb_vsyf9w0000gn/T/ipykernel_5136/1666610618.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df.columns = df.columns.str.replace('.', '-')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None               4996\n",
       "__label__Sexism     672\n",
       "Name: Sexism, dtype: int64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace \".\" with \"-\"\n",
    "df.columns = df.columns.str.replace('.', '-')\n",
    "\n",
    "cols = df.columns\n",
    "cols = ['Hate-speech', 'Sexism', 'Body', 'Racism', 'Ideology', 'Homophobia', 'Origin', 'Religion', 'Health', 'OtherLifestyle', 'Aborting-women', 'Agnostic', 'Argentines', 'Asians', 'Autists', 'Black-Women', 'Blond-women', 'Brazilians-women', 'Chinese', 'Criminals', 'Egyptians', 'Fat-people', 'Football-players-women', 'Gamers', 'Homeless', 'Homeless-women', 'Indigenous', 'Iranians', 'Japaneses', 'Jews', 'Jornalists', 'Latins', 'Left-wing-ideology', 'Men-Feminists', 'Mexicans', 'Muslims-women', 'Nordestines', 'Old-people', 'Polyamorous', 'Poor-people', 'Rural-people', 'Russians', 'Sertanejos', 'Street-artist', 'Ucranians', 'Vegetarians', 'White-people', 'Young-people', 'Old-women', 'Ugly-people', 'Venezuelans', 'Angolans', 'Black-people', 'Disabled-people', 'Fat-women', 'Feminists', 'Gays', 'Immigrants', 'Islamists', 'Lesbians', 'Men', 'Muslims', 'Refugees', 'Trans-women', 'Travestis', 'Women', 'Bissexuals', 'Transexuals', 'Ugly-women', 'Thin-women', 'Arabic', 'East-europeans', 'Africans', 'South-Americans', 'Brazilians', 'Migrants', 'Homossexuals', 'Thin-people', 'Ageing']\n",
    "\n",
    "def label_value(value, col):\n",
    "    label = f'__label__{col}'\n",
    "    if value == 1:\n",
    "        return label\n",
    "    else:\n",
    "        return \"None\"\n",
    "    \n",
    "for i in cols:\n",
    "    df[i] = df[i].apply(label_value, args=(i,))\n",
    "    \n",
    "df.Sexism.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d2200ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_total'] = df[cols].agg(''.join, axis=1)\n",
    "df['label_total'] = df['label_total'].str.replace(\"None\", \"\")\n",
    "df['label_total'] = df['label_total'].str.replace(\"__label\", \" __label\")\n",
    "df['label_total'] = df['label_total'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "35e18d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like not all data has a label\n",
    "# this function identify the one who got a label, and the one who don't\n",
    "def identify_label(value):\n",
    "    if \"__label\" in value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "17a8bbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4440\n",
       "1    1228\n",
       "Name: label_binary, dtype: int64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_binary'] = df['label_total'].apply(identify_label)\n",
    "df['label_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ad67386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['extremeText_label'] = df['label_total'].astype(str) + df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e2598d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   come morde marimbondo\n",
       "1                                           pinto orgulho\n",
       "2             merda crepúsculo cinebiografia chuck norris\n",
       "3                         tapa bundinha cotovelada costas\n",
       "4       __label__Hate-speech __label__Sexism __label__...\n",
       "                              ...                        \n",
       "5663    __label__Hate-speech __label__Homophobia __lab...\n",
       "5664    __label__Hate-speech __label__Homophobia __lab...\n",
       "5665    __label__Hate-speech __label__Homophobia __lab...\n",
       "5666    __label__Hate-speech __label__Homophobia __lab...\n",
       "5667    __label__Hate-speech __label__Homophobia __lab...\n",
       "Name: extremeText_label, Length: 5668, dtype: object"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['extremeText_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ab09c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n",
      "4440\n"
     ]
    }
   ],
   "source": [
    "print(len(labeled_data))\n",
    "print(len(non_labeled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "d704e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = list(df[df['label_binary'] == 1]['extremeText_label'].values)\n",
    "non_labeled_data = list(df[df['label_binary'] == 0]['extremeText_label'].values)\n",
    "\n",
    "train_data = labeled_data[0:1000] +  non_labeled_data[0:3140]\n",
    "test_data = labeled_data[1000:] +  non_labeled_data[3140:]\n",
    "\n",
    "\n",
    "with open('./data/train_hs.txt', 'a') as the_file:\n",
    "    for item in train_data:\n",
    "        the_file.write(f\"{item}\\n\")\n",
    "        \n",
    "with open('./data/test_hs.txt', 'a') as the_file:\n",
    "    for item in test_data:\n",
    "        the_file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00d043",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "84fd4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE train, test split\n",
    "# create a new one with this: http://scikit.ml/stratification.html\n",
    "# X = df.copy()\n",
    "# del X['Hate.speech']\n",
    "# del X['text']\n",
    "\n",
    "# y = df['Hate.speech']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "2ba53201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split data\n",
    "# https://datascience.stackexchange.com/questions/45174/how-to-use-sklearn-train-test-split-to-stratify-data-for-multi-label-classificat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ed6141be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = make_multilabel_classification(n_samples=300, n_classes=100, n_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "c27bdb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 20)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "2496c4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 100)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6306cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = iterative_train_test_split(X,Y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "d782eda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 3., ..., 3., 2., 2.],\n",
       "       [0., 6., 2., ..., 7., 0., 4.],\n",
       "       [2., 1., 2., ..., 1., 4., 3.],\n",
       "       ...,\n",
       "       [2., 1., 4., ..., 3., 2., 2.],\n",
       "       [2., 9., 2., ..., 0., 2., 2.],\n",
       "       [4., 1., 7., ..., 7., 3., 5.]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c59e68",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "https://github.com/automated-library/ICPC_2022_Automated-Identification-of-Libraries-from-Vulnerability-Data/blob/main/extremeText/extremetext_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "29d6d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/automated-library/ICPC_2022_Automated-Identification-of-Libraries-from-Vulnerability-Data/blob/main/extremeText/extremetext_train.py\n",
    "\n",
    "def model_training(train_data):\n",
    "    # train_supervised uses the same arguments and defaults as the fastText/extremeText cli\n",
    "\n",
    "    print(\"Supervised Training\")\n",
    "    # default supervised training\n",
    "    # model = extremeText.train_supervised(\n",
    "    #     input=train_data, epoch=100, lr=1.0, wordNgrams=2, verbose=3, minCount=1,\n",
    "    # )\n",
    "\n",
    "    # paper supervised training\n",
    "    model = extremeText.train_supervised(\n",
    "        input=train_data, epoch=300, lr=0.05, verbose=3, wordNgrams=2, minCount=1, l2=0.003, arity=2, dim=100, tfidfWeights=True\n",
    "    )\n",
    "    model.save_model(\"./model/xt_supervised.bin\")\n",
    "    return model\n",
    "\n",
    "    # print(\"Quantization\")\n",
    "    #\n",
    "    # model.quantize(input=train_data, qnorm=True, retrain=True, cutoff=100000)\n",
    "    #\n",
    "    # model.save_model(\"model/xt_quantized.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "6fa84fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training(train_data = \"./dataset/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "0e428021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "  Model: sup, loss: softmax\n",
      "  Features: TF-IDF weights, buckets: 2000000\n",
      "\n",
      "  Update: SGD, lr: 0.050000, L2: 0.003000, dims: 100, epochs: 300, neg: 5\n",
      "Reading input file ...\n",
      "Read 0M words\n",
      "Number of documents: 45980\n",
      "Number of words: 23613\n",
      "Number of labels: 1533\n",
      "  Input: 2023613 x 100 (771M)\n",
      "Setting up loss layer ...\n",
      "  Output: 1533 x 100 (0M)\n",
      "Starting 12 threads ...\n",
      "Progress:  89.6% words/sec/thread:   80319 lr:  0.005221 loss:  2.069875 ETA:   0h 0m200 loss:  5.058578 ETA:   0h 2ms/sec/thread:   82421 lr:  0.044019 loss:  4.996305 ETA:   0h 2mlr:  0.043788 loss:  4.890483 ETA:   0h 2m12.6% words/sec/thread:   80965 lr:  0.043681 loss:  4.837677 ETA:   0h 2m.757978 ETA:   0h 2m:   80652 lr:  0.043338 loss:  4.670217 ETA:   0h 2m   0h 2m  80734 lr:  0.041777 loss:  4.266115 ETA:   0h 2mloss:  4.141429 ETA:   0h 2m 0.040655 loss:  4.060303 ETA:   0h 2m   0h 1ms/sec/thread:   80367 lr:  0.013803 loss:  2.229556 ETA:   0h 0mA:   0h 0mlr:  0.013476 loss:  2.219936 ETA:   0h 0ms/sec/thread:   80339 lr:  0.012069 loss:  2.191975 ETA:   0h 0mA:   0h 0mlr:  0.011753 loss:  2.183743 ETA:   0h 0m.175659 ETA:   0h 0m874 loss:  2.146556 ETA:   0h 0m613 loss:  2.140537 ETA:   0h 0m183 loss:  2.133382 ETA:   0h 0ms/sec/thread:   80265 lr:  0.008976 loss:  2.131807 ETA:   0h 0mA:   0h 0mlr:  0.008576 loss:  2.125003 ETA:   0h 0m83.2% words/sec/thread:   80296 lr:  0.008403 loss:  2.121673 ETA:   0h 0m.118514 ETA:   0h 0m:   80173 lr:  0.008100 loss:  2.114859 ETA:   0h 0mrogress:  84.1% words/sec/thread:   80103 lr:  0.007962 loss:  2.111788 ETA:   0h 0m9 loss:  2.107984 ETA:   0h 0msec/thread:   80121 lr:  0.007588 loss:  2.104331 ETA:   0h 0m.1% words/sec/thread:   80303 lr:  0.006937 loss:  2.096902 ETA:   0h 0m"
     ]
    }
   ],
   "source": [
    "model = model_training(train_data = \"./data/train_hs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2fc8e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "626 ETA:   0h 0m\r",
      "Progress:  89.6% words/sec/thread:   80316 lr:  0.005204 loss:  2.069346 ETA:   0h 0m\r",
      "Progress:  89.6% words/sec/thread:   80325 lr:  0.005182 loss:  2.068992 ETA:   0h 0m\r",
      "Progress:  89.7% words/sec/thread:   80324 lr:  0.005167 loss:  2.068717 ETA:   0h 0m\r",
      "Progress:  89.7% words/sec/thread:   80309 lr:  0.005159 loss:  2.068431 ETA:   0h 0m\r",
      "Progress:  89.7% words/sec/thread:   80309 lr:  0.005142 loss:  2.068134 ETA:   0h 0m\r",
      "Progress:  89.8% words/sec/thread:   80340 lr:  0.005107 loss:  2.067903 ETA:   0h 0m\r",
      "Progress:  89.8% words/sec/thread:   80337 lr:  0.005093 loss:  2.067703 ETA:   0h 0m\r",
      "Progress:  89.8% words/sec/thread:   80335 lr:  0.005078 loss:  2.067493 ETA:   0h 0m\r",
      "Progress:  89.9% words/sec/thread:   80331 lr:  0.005064 loss:  2.067251 ETA:   0h 0m\r",
      "Progress:  89.9% words/sec/thread:   80331 lr:  0.005047 loss:  2.067005 ETA:   0h 0m\r",
      "Progress:  89.9% words/sec/thread:   80316 lr:  0.005040 loss:  2.066681 ETA:   0h 0m\r",
      "Progress:  90.0% words/sec/thread:   80326 lr:  0.005018 loss:  2.066279 ETA:   0h 0m\r",
      "Progress:  90.0% words/sec/thread:   80350 lr:  0.004989 loss:  2.066004 ETA:   0h 0m\r",
      "Progress:  90.1% words/sec/thread:   80348 lr:  0.004973 loss:  2.065722 ETA:   0h 0m\r",
      "Progress:  90.1% words/sec/thread:   80332 lr:  0.004965 loss:  2.065530 ETA:   0h 0m\r",
      "Progress:  90.1% words/sec/thread:   80315 lr:  0.004958 loss:  2.065317 ETA:   0h 0m\r",
      "Progress:  90.1% words/sec/thread:   80327 lr:  0.004935 loss:  2.065125 ETA:   0h 0m\r",
      "Progress:  90.1% words/sec/thread:   80312 lr:  0.004926 loss:  2.064877 ETA:   0h 0m\r",
      "Progress:  90.2% words/sec/thread:   80310 lr:  0.004910 loss:  2.064565 ETA:   0h 0m\r",
      "Progress:  90.2% words/sec/thread:   80320 lr:  0.004888 loss:  2.064167 ETA:   0h 0m\r",
      "Progress:  90.3% words/sec/thread:   80319 lr:  0.004871 loss:  2.063881 ETA:   0h 0m\r",
      "Progress:  90.3% words/sec/thread:   80308 lr:  0.004863 loss:  2.063620 ETA:   0h 0m\r",
      "Progress:  90.3% words/sec/thread:   80309 lr:  0.004848 loss:  2.063403 ETA:   0h 0m\r",
      "Progress:  90.3% words/sec/thread:   80296 lr:  0.004841 loss:  2.063235 ETA:   0h 0m\r",
      "Progress:  90.3% words/sec/thread:   80283 lr:  0.004833 loss:  2.063042 ETA:   0h 0m\r",
      "Progress:  90.4% words/sec/thread:   80284 lr:  0.004817 loss:  2.062875 ETA:   0h 0m\r",
      "Progress:  90.4% words/sec/thread:   80270 lr:  0.004809 loss:  2.062580 ETA:   0h 0m\r",
      "Progress:  90.4% words/sec/thread:   80282 lr:  0.004786 loss:  2.062275 ETA:   0h 0m\r",
      "Progress:  90.4% words/sec/thread:   80268 lr:  0.004777 loss:  2.061942 ETA:   0h 0m\r",
      "Progress:  90.5% words/sec/thread:   80254 lr:  0.004768 loss:  2.061677 ETA:   0h 0m\r",
      "Progress:  90.5% words/sec/thread:   80240 lr:  0.004759 loss:  2.061366 ETA:   0h 0m\r",
      "Progress:  90.5% words/sec/thread:   80228 lr:  0.004750 loss:  2.061143 ETA:   0h 0m\r",
      "Progress:  90.5% words/sec/thread:   80235 lr:  0.004730 loss:  2.060898 ETA:   0h 0m\r",
      "Progress:  90.6% words/sec/thread:   80231 lr:  0.004724 loss:  2.060850 ETA:   0h 0m\r",
      "Progress:  90.6% words/sec/thread:   80231 lr:  0.004709 loss:  2.060689 ETA:   0h 0m\r",
      "Progress:  90.6% words/sec/thread:   80227 lr:  0.004696 loss:  2.060468 ETA:   0h 0m\r",
      "Progress:  90.6% words/sec/thread:   80214 lr:  0.004689 loss:  2.060177 ETA:   0h 0m\r",
      "Progress:  90.7% words/sec/thread:   80211 lr:  0.004673 loss:  2.059841 ETA:   0h 0m\r",
      "Progress:  90.7% words/sec/thread:   80208 lr:  0.004657 loss:  2.059491 ETA:   0h 0m\r",
      "Progress:  90.7% words/sec/thread:   80195 lr:  0.004648 loss:  2.059178 ETA:   0h 0m\r",
      "Progress:  90.7% words/sec/thread:   80181 lr:  0.004640 loss:  2.058930 ETA:   0h 0m\r",
      "Progress:  90.8% words/sec/thread:   80193 lr:  0.004617 loss:  2.058739 ETA:   0h 0m\r",
      "Progress:  90.8% words/sec/thread:   80204 lr:  0.004594 loss:  2.058511 ETA:   0h 0m\r",
      "Progress:  90.8% words/sec/thread:   80189 lr:  0.004586 loss:  2.058286 ETA:   0h 0m\r",
      "Progress:  90.8% words/sec/thread:   80174 lr:  0.004578 loss:  2.058036 ETA:   0h 0m\r",
      "Progress:  90.9% words/sec/thread:   80162 lr:  0.004569 loss:  2.057702 ETA:   0h 0m\r",
      "Progress:  90.9% words/sec/thread:   80165 lr:  0.004550 loss:  2.057366 ETA:   0h 0m\r",
      "Progress:  90.9% words/sec/thread:   80157 lr:  0.004538 loss:  2.057102 ETA:   0h 0m\r",
      "Progress:  91.0% words/sec/thread:   80157 lr:  0.004523 loss:  2.056817 ETA:   0h 0m\r",
      "Progress:  91.0% words/sec/thread:   80143 lr:  0.004515 loss:  2.056647 ETA:   0h 0m\r",
      "Progress:  91.0% words/sec/thread:   80155 lr:  0.004492 loss:  2.056410 ETA:   0h 0m\r",
      "Progress:  91.0% words/sec/thread:   80155 lr:  0.004475 loss:  2.056235 ETA:   0h 0m\r",
      "Progress:  91.1% words/sec/thread:   80141 lr:  0.004467 loss:  2.056004 ETA:   0h 0m\r",
      "Progress:  91.1% words/sec/thread:   80128 lr:  0.004458 loss:  2.055687 ETA:   0h 0m\r",
      "Progress:  91.1% words/sec/thread:   80129 lr:  0.004441 loss:  2.055382 ETA:   0h 0m\r",
      "Progress:  91.1% words/sec/thread:   80123 lr:  0.004428 loss:  2.055084 ETA:   0h 0m\r",
      "Progress:  91.2% words/sec/thread:   80112 lr:  0.004416 loss:  2.054803 ETA:   0h 0m\r",
      "Progress:  91.2% words/sec/thread:   80111 lr:  0.004398 loss:  2.054476 ETA:   0h 0m\r",
      "Progress:  91.2% words/sec/thread:   80109 lr:  0.004381 loss:  2.054067 ETA:   0h 0m\r",
      "Progress:  91.3% words/sec/thread:   80111 lr:  0.004365 loss:  2.053781 ETA:   0h 0m\r",
      "Progress:  91.3% words/sec/thread:   80110 lr:  0.004348 loss:  2.053501 ETA:   0h 0m\r",
      "Progress:  91.3% words/sec/thread:   80111 lr:  0.004332 loss:  2.053307 ETA:   0h 0m\r",
      "Progress:  91.4% words/sec/thread:   80112 lr:  0.004315 loss:  2.053013 ETA:   0h 0m\r",
      "Progress:  91.4% words/sec/thread:   80112 lr:  0.004299 loss:  2.052666 ETA:   0h 0m\r",
      "Progress:  91.4% words/sec/thread:   80098 lr:  0.004290 loss:  2.052437 ETA:   0h 0m\r",
      "Progress:  91.4% words/sec/thread:   80085 lr:  0.004281 loss:  2.052127 ETA:   0h 0m\r",
      "Progress:  91.5% words/sec/thread:   80107 lr:  0.004252 loss:  2.051834 ETA:   0h 0m\r",
      "Progress:  91.6% words/sec/thread:   80129 lr:  0.004223 loss:  2.051516 ETA:   0h 0m\r",
      "Progress:  91.6% words/sec/thread:   80127 lr:  0.004207 loss:  2.051261 ETA:   0h 0m\r",
      "Progress:  91.6% words/sec/thread:   80113 lr:  0.004200 loss:  2.050954 ETA:   0h 0m\r",
      "Progress:  91.6% words/sec/thread:   80113 lr:  0.004183 loss:  2.050767 ETA:   0h 0m\r",
      "Progress:  91.7% words/sec/thread:   80112 lr:  0.004167 loss:  2.050413 ETA:   0h 0m\r",
      "Progress:  91.7% words/sec/thread:   80125 lr:  0.004143 loss:  2.050112 ETA:   0h 0m\r",
      "Progress:  91.7% words/sec/thread:   80121 lr:  0.004129 loss:  2.049848 ETA:   0h 0m\r",
      "Progress:  91.8% words/sec/thread:   80121 lr:  0.004112 loss:  2.049541 ETA:   0h 0m\r",
      "Progress:  91.8% words/sec/thread:   80126 lr:  0.004093 loss:  2.049278 ETA:   0h 0m\r",
      "Progress:  91.9% words/sec/thread:   80143 lr:  0.004066 loss:  2.048942 ETA:   0h 0m\r",
      "Progress:  91.9% words/sec/thread:   80126 lr:  0.004057 loss:  2.048661 ETA:   0h 0m\r",
      "Progress:  91.9% words/sec/thread:   80125 lr:  0.004041 loss:  2.048415 ETA:   0h 0m\r",
      "Progress:  92.0% words/sec/thread:   80127 lr:  0.004023 loss:  2.048114 ETA:   0h 0m\r",
      "Progress:  92.0% words/sec/thread:   80147 lr:  0.003995 loss:  2.047844 ETA:   0h 0m\r",
      "Progress:  92.1% words/sec/thread:   80170 lr:  0.003966 loss:  2.047501 ETA:   0h 0m\r",
      "Progress:  92.1% words/sec/thread:   80162 lr:  0.003955 loss:  2.047294 ETA:   0h 0m\r",
      "Progress:  92.1% words/sec/thread:   80162 lr:  0.003938 loss:  2.047035 ETA:   0h 0m\r",
      "Progress:  92.2% words/sec/thread:   80163 lr:  0.003922 loss:  2.046866 ETA:   0h 0m\r",
      "Progress:  92.2% words/sec/thread:   80185 lr:  0.003893 loss:  2.046630 ETA:   0h 0m\r",
      "Progress:  92.3% words/sec/thread:   80192 lr:  0.003873 loss:  2.046499 ETA:   0h 0m\r",
      "Progress:  92.3% words/sec/thread:   80200 lr:  0.003851 loss:  2.046328 ETA:   0h 0m\r",
      "Progress:  92.3% words/sec/thread:   80209 lr:  0.003830 loss:  2.046068 ETA:   0h 0m\r",
      "Progress:  92.4% words/sec/thread:   80221 lr:  0.003807 loss:  2.045968 ETA:   0h 0m\r",
      "Progress:  92.4% words/sec/thread:   80221 lr:  0.003791 loss:  2.045864 ETA:   0h 0m\r",
      "Progress:  92.5% words/sec/thread:   80220 lr:  0.003775 loss:  2.045786 ETA:   0h 0m\r",
      "Progress:  92.5% words/sec/thread:   80227 lr:  0.003755 loss:  2.045679 ETA:   0h 0m\r",
      "Progress:  92.5% words/sec/thread:   80225 lr:  0.003739 loss:  2.045659 ETA:   0h 0m\r",
      "Progress:  92.6% words/sec/thread:   80263 lr:  0.003701 loss:  2.045522 ETA:   0h 0m\r",
      "Progress:  92.6% words/sec/thread:   80259 lr:  0.003686 loss:  2.045195 ETA:   0h 0m\r",
      "Progress:  92.7% words/sec/thread:   80255 lr:  0.003669 loss:  2.045102 ETA:   0h 0m\r",
      "Progress:  92.7% words/sec/thread:   80254 lr:  0.003653 loss:  2.044950 ETA:   0h 0m\r",
      "Progress:  92.7% words/sec/thread:   80271 lr:  0.003626 loss:  2.044860 ETA:   0h 0m\r",
      "Progress:  92.8% words/sec/thread:   80263 lr:  0.003614 loss:  2.044731 ETA:   0h 0m\r",
      "Progress:  92.8% words/sec/thread:   80274 lr:  0.003592 loss:  2.044668 ETA:   0h 0m\r",
      "Progress:  92.9% words/sec/thread:   80282 lr:  0.003571 loss:  2.044389 ETA:   0h 0m\r",
      "Progress:  92.9% words/sec/thread:   80299 lr:  0.003546 loss:  2.044074 ETA:   0h 0m\r",
      "Progress:  92.9% words/sec/thread:   80308 lr:  0.003526 loss:  2.043908 ETA:   0h 0m\r",
      "Progress:  93.0% words/sec/thread:   80294 lr:  0.003518 loss:  2.043592 ETA:   0h 0m\r",
      "Progress:  93.0% words/sec/thread:   80281 lr:  0.003510 loss:  2.043385 ETA:   0h 0m\r",
      "Progress:  93.0% words/sec/thread:   80295 lr:  0.003487 loss:  2.043211 ETA:   0h 0m\r",
      "Progress:  93.1% words/sec/thread:   80305 lr:  0.003466 loss:  2.043024 ETA:   0h 0m\r",
      "Progress:  93.1% words/sec/thread:   80311 lr:  0.003447 loss:  2.042786 ETA:   0h 0m\r",
      "Progress:  93.1% words/sec/thread:   80303 lr:  0.003436 loss:  2.042605 ETA:   0h 0m\r",
      "Progress:  93.1% words/sec/thread:   80291 lr:  0.003429 loss:  2.042334 ETA:   0h 0m\r",
      "Progress:  93.2% words/sec/thread:   80314 lr:  0.003401 loss:  2.042049 ETA:   0h 0m\r",
      "Progress:  93.2% words/sec/thread:   80301 lr:  0.003394 loss:  2.041822 ETA:   0h 0m\r",
      "Progress:  93.2% words/sec/thread:   80301 lr:  0.003379 loss:  2.041552 ETA:   0h 0m\r",
      "Progress:  93.3% words/sec/thread:   80288 lr:  0.003372 loss:  2.041325 ETA:   0h 0m\r",
      "Progress:  93.3% words/sec/thread:   80320 lr:  0.003337 loss:  2.041112 ETA:   0h 0m\r",
      "Progress:  93.4% words/sec/thread:   80329 lr:  0.003314 loss:  2.040906 ETA:   0h 0m\r",
      "Progress:  93.4% words/sec/thread:   80316 lr:  0.003307 loss:  2.040736 ETA:   0h 0m\r",
      "Progress:  93.4% words/sec/thread:   80326 lr:  0.003284 loss:  2.040524 ETA:   0h 0m\r",
      "Progress:  93.4% words/sec/thread:   80312 lr:  0.003276 loss:  2.040229 ETA:   0h 0m\r",
      "Progress:  93.5% words/sec/thread:   80311 lr:  0.003261 loss:  2.039893 ETA:   0h 0m\r",
      "Progress:  93.5% words/sec/thread:   80296 lr:  0.003253 loss:  2.039649 ETA:   0h 0m\r",
      "Progress:  93.6% words/sec/thread:   80320 lr:  0.003223 loss:  2.039351 ETA:   0h 0m\r",
      "Progress:  93.6% words/sec/thread:   80328 lr:  0.003202 loss:  2.039129 ETA:   0h 0m\r",
      "Progress:  93.6% words/sec/thread:   80326 lr:  0.003187 loss:  2.038945 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80323 lr:  0.003172 loss:  2.038734 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80314 lr:  0.003167 loss:  2.038620 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80311 lr:  0.003159 loss:  2.038438 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80311 lr:  0.003148 loss:  2.038311 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80304 lr:  0.003144 loss:  2.038040 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80303 lr:  0.003141 loss:  2.037930 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80309 lr:  0.003130 loss:  2.037868 ETA:   0h 0m\r",
      "Progress:  93.7% words/sec/thread:   80301 lr:  0.003125 loss:  2.037717 ETA:   0h 0m\r",
      "Progress:  93.8% words/sec/thread:   80302 lr:  0.003110 loss:  2.037446 ETA:   0h 0m\r",
      "Progress:  93.8% words/sec/thread:   80298 lr:  0.003096 loss:  2.037209 ETA:   0h 0m\r",
      "Progress:  93.8% words/sec/thread:   80298 lr:  0.003080 loss:  2.036994 ETA:   0h 0m\r",
      "Progress:  93.9% words/sec/thread:   80297 lr:  0.003063 loss:  2.036749 ETA:   0h 0m\r",
      "Progress:  93.9% words/sec/thread:   80281 lr:  0.003055 loss:  2.036549 ETA:   0h 0m\r",
      "Progress:  93.9% words/sec/thread:   80266 lr:  0.003046 loss:  2.036319 ETA:   0h 0m\r",
      "Progress:  93.9% words/sec/thread:   80265 lr:  0.003028 loss:  2.035966 ETA:   0h 0m\r",
      "Progress:  94.0% words/sec/thread:   80275 lr:  0.003004 loss:  2.035640 ETA:   0h 0m\r",
      "Progress:  94.0% words/sec/thread:   80262 lr:  0.002995 loss:  2.035337 ETA:   0h 0m\r",
      "Progress:  94.0% words/sec/thread:   80247 lr:  0.002985 loss:  2.035105 ETA:   0h 0m\r",
      "Progress:  94.0% words/sec/thread:   80232 lr:  0.002975 loss:  2.034862 ETA:   0h 0m\r",
      "Progress:  94.1% words/sec/thread:   80219 lr:  0.002966 loss:  2.034576 ETA:   0h 0m\r",
      "Progress:  94.1% words/sec/thread:   80226 lr:  0.002944 loss:  2.034388 ETA:   0h 0m\r",
      "Progress:  94.1% words/sec/thread:   80211 lr:  0.002934 loss:  2.034175 ETA:   0h 0m\r",
      "Progress:  94.2% words/sec/thread:   80217 lr:  0.002912 loss:  2.033789 ETA:   0h 0m\r",
      "Progress:  94.2% words/sec/thread:   80215 lr:  0.002895 loss:  2.033502 ETA:   0h 0m\r",
      "Progress:  94.2% words/sec/thread:   80214 lr:  0.002878 loss:  2.033204 ETA:   0h 0m\r",
      "Progress:  94.3% words/sec/thread:   80197 lr:  0.002869 loss:  2.032912 ETA:   0h 0m\r",
      "Progress:  94.3% words/sec/thread:   80183 lr:  0.002861 loss:  2.032701 ETA:   0h 0m\r",
      "Progress:  94.3% words/sec/thread:   80193 lr:  0.002837 loss:  2.032503 ETA:   0h 0m\r",
      "Progress:  94.4% words/sec/thread:   80191 lr:  0.002821 loss:  2.032299 ETA:   0h 0m\r",
      "Progress:  94.4% words/sec/thread:   80194 lr:  0.002807 loss:  2.032116 ETA:   0h 0m\r",
      "Progress:  94.4% words/sec/thread:   80182 lr:  0.002800 loss:  2.031845 ETA:   0h 0m\r",
      "Progress:  94.4% words/sec/thread:   80170 lr:  0.002793 loss:  2.031535 ETA:   0h 0m\r",
      "Progress:  94.4% words/sec/thread:   80168 lr:  0.002778 loss:  2.031298 ETA:   0h 0m\r",
      "Progress:  94.5% words/sec/thread:   80166 lr:  0.002762 loss:  2.031231 ETA:   0h 0m\r",
      "Progress:  94.5% words/sec/thread:   80159 lr:  0.002750 loss:  2.030868 ETA:   0h 0m\r",
      "Progress:  94.5% words/sec/thread:   80150 lr:  0.002740 loss:  2.030676 ETA:   0h 0m\r",
      "Progress:  94.5% words/sec/thread:   80145 lr:  0.002726 loss:  2.030489 ETA:   0h 0m\r",
      "Progress:  94.6% words/sec/thread:   80150 lr:  0.002708 loss:  2.030321 ETA:   0h 0m\r",
      "Progress:  94.6% words/sec/thread:   80150 lr:  0.002692 loss:  2.030123 ETA:   0h 0m\r",
      "Progress:  94.6% words/sec/thread:   80137 lr:  0.002683 loss:  2.029916 ETA:   0h 0m\r",
      "Progress:  94.7% words/sec/thread:   80124 lr:  0.002675 loss:  2.029573 ETA:   0h 0m\r",
      "Progress:  94.7% words/sec/thread:   80112 lr:  0.002665 loss:  2.029275 ETA:   0h 0m\r",
      "Progress:  94.7% words/sec/thread:   80126 lr:  0.002641 loss:  2.029059 ETA:   0h 0m\r",
      "Progress:  94.7% words/sec/thread:   80112 lr:  0.002632 loss:  2.028802 ETA:   0h 0m\r",
      "Progress:  94.8% words/sec/thread:   80112 lr:  0.002616 loss:  2.028508 ETA:   0h 0m\r",
      "Progress:  94.8% words/sec/thread:   80098 lr:  0.002608 loss:  2.028190 ETA:   0h 0m\r",
      "Progress:  94.8% words/sec/thread:   80111 lr:  0.002583 loss:  2.027929 ETA:   0h 0m\r",
      "Progress:  94.9% words/sec/thread:   80109 lr:  0.002566 loss:  2.027624 ETA:   0h 0m\r",
      "Progress:  94.9% words/sec/thread:   80107 lr:  0.002549 loss:  2.027369 ETA:   0h 0m\r",
      "Progress:  94.9% words/sec/thread:   80106 lr:  0.002531 loss:  2.027014 ETA:   0h 0m\r",
      "Progress:  95.0% words/sec/thread:   80105 lr:  0.002515 loss:  2.026742 ETA:   0h 0m\r",
      "Progress:  95.0% words/sec/thread:   80095 lr:  0.002505 loss:  2.026426 ETA:   0h 0m\r",
      "Progress:  95.0% words/sec/thread:   80096 lr:  0.002488 loss:  2.026276 ETA:   0h 0m\r",
      "Progress:  95.1% words/sec/thread:   80121 lr:  0.002456 loss:  2.025929 ETA:   0h 0m\r",
      "Progress:  95.1% words/sec/thread:   80125 lr:  0.002438 loss:  2.025634 ETA:   0h 0m\r",
      "Progress:  95.2% words/sec/thread:   80124 lr:  0.002422 loss:  2.025439 ETA:   0h 0m\r",
      "Progress:  95.2% words/sec/thread:   80124 lr:  0.002406 loss:  2.025213 ETA:   0h 0m\r",
      "Progress:  95.2% words/sec/thread:   80110 lr:  0.002398 loss:  2.024876 ETA:   0h 0m\r",
      "Progress:  95.2% words/sec/thread:   80113 lr:  0.002379 loss:  2.024585 ETA:   0h 0m\r",
      "Progress:  95.3% words/sec/thread:   80122 lr:  0.002358 loss:  2.024335 ETA:   0h 0m\r",
      "Progress:  95.3% words/sec/thread:   80122 lr:  0.002341 loss:  2.024026 ETA:   0h 0m\r",
      "Progress:  95.4% words/sec/thread:   80130 lr:  0.002321 loss:  2.023786 ETA:   0h 0m\r",
      "Progress:  95.4% words/sec/thread:   80128 lr:  0.002305 loss:  2.023472 ETA:   0h 0m\r",
      "Progress:  95.4% words/sec/thread:   80126 lr:  0.002289 loss:  2.023261 ETA:   0h 0m\r",
      "Progress:  95.5% words/sec/thread:   80126 lr:  0.002273 loss:  2.022970 ETA:   0h 0m\r",
      "Progress:  95.5% words/sec/thread:   80139 lr:  0.002249 loss:  2.022730 ETA:   0h 0m\r",
      "Progress:  95.5% words/sec/thread:   80132 lr:  0.002235 loss:  2.022407 ETA:   0h 0m\r",
      "Progress:  95.6% words/sec/thread:   80146 lr:  0.002208 loss:  2.022090 ETA:   0h 0m\r",
      "Progress:  95.6% words/sec/thread:   80168 lr:  0.002177 loss:  2.021857 ETA:   0h 0m\r",
      "Progress:  95.7% words/sec/thread:   80169 lr:  0.002160 loss:  2.021591 ETA:   0h 0m\r",
      "Progress:  95.7% words/sec/thread:   80163 lr:  0.002147 loss:  2.021435 ETA:   0h 0m\r",
      "Progress:  95.7% words/sec/thread:   80163 lr:  0.002130 loss:  2.021221 ETA:   0h 0m\r",
      "Progress:  95.8% words/sec/thread:   80204 lr:  0.002089 loss:  2.021132 ETA:   0h 0m\r",
      "Progress:  95.8% words/sec/thread:   80191 lr:  0.002081 loss:  2.020841 ETA:   0h 0m\r",
      "Progress:  95.9% words/sec/thread:   80191 lr:  0.002064 loss:  2.020611 ETA:   0h 0m\r",
      "Progress:  95.9% words/sec/thread:   80200 lr:  0.002043 loss:  2.020618 ETA:   0h 0m\r",
      "Progress:  96.0% words/sec/thread:   80219 lr:  0.002015 loss:  2.020449 ETA:   0h 0m\r",
      "Progress:  96.0% words/sec/thread:   80229 lr:  0.001993 loss:  2.020428 ETA:   0h 0m\r",
      "Progress:  96.0% words/sec/thread:   80230 lr:  0.001975 loss:  2.020264 ETA:   0h 0m\r",
      "Progress:  96.1% words/sec/thread:   80240 lr:  0.001953 loss:  2.020309 ETA:   0h 0m\r",
      "Progress:  96.1% words/sec/thread:   80250 lr:  0.001931 loss:  2.020094 ETA:   0h 0m\r",
      "Progress:  96.2% words/sec/thread:   80251 lr:  0.001914 loss:  2.019798 ETA:   0h 0m\r",
      "Progress:  96.2% words/sec/thread:   80256 lr:  0.001894 loss:  2.019815 ETA:   0h 0m\r",
      "Progress:  96.2% words/sec/thread:   80255 lr:  0.001878 loss:  2.019658 ETA:   0h 0m\r",
      "Progress:"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('__label__Hate-speech',), array([-0.36828128]))"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:   80270 lr:  0.000000 loss:  1.995195 ETA:   0h 0m\n",
      "Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model.predict('O mundo das sapatao é mais ligado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c0d87c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test ...\n",
      "  Model: sup, loss: softmax\n",
      "  Features: TF-IDF weights, buckets: 2000000\n",
      "\n",
      "  Update: SGD, lr: 0.050000, L2: 0.003000, dims: 100, epochs: 300, neg: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3164, 0.9860935524652339, 0.2243797195253506, 0.07110241356816699)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('./data/test_hs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f0a99479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test ...\n",
      "  Model: sup, loss: softmax\n",
      "  Features: TF-IDF weights, buckets: 2000000\n",
      "\n",
      "  Update: SGD, lr: 0.050000, L2: 0.003000, dims: 100, epochs: 300, neg: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3164, 0.9632321955330805, 0.6575332614167566, 0.20939334637964774)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('./data/test_hs.txt', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "c08c0102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test ...\n",
      "  Model: sup, loss: softmax\n",
      "  Features: TF-IDF weights, buckets: 2000000\n",
      "\n",
      "  Update: SGD, lr: 0.050000, L2: 0.003000, dims: 100, epochs: 300, neg: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3164, 0.7428571428571429, 0.8451636102121539, 0.25701239399869535)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('./data/test_hs.txt', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "dae69432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__WomenTem',), array([-6.70372248]))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('bolsonaro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c57d49cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18418"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cb7b3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_4words = []\n",
    "for word in model.get_words():\n",
    "    if len(word) > 3:\n",
    "        l_4words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "08b6325a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17034"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_4words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "1cabf65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'https',\n",
       " 'mulher',\n",
       " 'para',\n",
       " 'mais',\n",
       " 'como',\n",
       " 'burra',\n",
       " 'gorda',\n",
       " 'pode',\n",
       " 'homem',\n",
       " 'isso',\n",
       " 'está',\n",
       " 'fufas',\n",
       " 'feia',\n",
       " 'essa',\n",
       " 'quem',\n",
       " 'gente',\n",
       " 'http',\n",
       " 'sapatão',\n",
       " 'mesmo',\n",
       " 'minha',\n",
       " 'muito',\n",
       " 'contra',\n",
       " 'aqui',\n",
       " 'quando',\n",
       " '#PNR',\n",
       " 'refugiados',\n",
       " 'fazer',\n",
       " 'mulheres',\n",
       " 'quer',\n",
       " 'esse',\n",
       " 'sobre',\n",
       " 'tudo',\n",
       " 'você',\n",
       " 'coisa',\n",
       " 'cara',\n",
       " 'ainda',\n",
       " 'pelo',\n",
       " 'agora',\n",
       " '@homemdeverdade',\n",
       " '#MulherDeVerdade',\n",
       " 'mundo',\n",
       " 'nada',\n",
       " 'gosta',\n",
       " 'racismo',\n",
       " '#Portugal',\n",
       " '#RenovarPortugal',\n",
       " 'pela',\n",
       " 'eles',\n",
       " 'sabe',\n",
       " 'sempre',\n",
       " 'sapatao',\n",
       " 'assim',\n",
       " 'menos',\n",
       " 'pessoas',\n",
       " 'acha',\n",
       " 'porque',\n",
       " 'toda',\n",
       " 'hoje',\n",
       " '@jpintocoelho60',\n",
       " 'tenho',\n",
       " 'nunca',\n",
       " 'seus',\n",
       " 'dizer',\n",
       " 'fala',\n",
       " 'fica',\n",
       " 'Trump',\n",
       " 'todo',\n",
       " 'feia,',\n",
       " '@rpsantos1970',\n",
       " 'fosse',\n",
       " 'melhor',\n",
       " 'falar',\n",
       " 'vida',\n",
       " 'nosso',\n",
       " 'branco',\n",
       " 'todos',\n",
       " 'anos',\n",
       " 'estão',\n",
       " 'quero',\n",
       " 'Como',\n",
       " 'depois',\n",
       " 'homens',\n",
       " 'país',\n",
       " 'feminista',\n",
       " 'hora',\n",
       " 'acho',\n",
       " 'gorda,',\n",
       " 'esta',\n",
       " '@direitafalando',\n",
       " '#JoaquinResponde',\n",
       " 'negro',\n",
       " 'esquerda',\n",
       " 'casa',\n",
       " 'Portugal',\n",
       " 'tempo',\n",
       " 'passar',\n",
       " 'entre',\n",
       " 'disse',\n",
       " 'grande',\n",
       " 'cada',\n",
       " 'livro',\n",
       " 'refugiados.',\n",
       " 'não,',\n",
       " 'podem',\n",
       " 'Quando',\n",
       " 'também',\n",
       " 'MULHER',\n",
       " 'antes',\n",
       " 'falando',\n",
       " 'chama',\n",
       " 'merda',\n",
       " 'outra',\n",
       " 'estar',\n",
       " 'nossa',\n",
       " 'entrar',\n",
       " 'chamar',\n",
       " 'vejo',\n",
       " 'onde',\n",
       " 'lugar',\n",
       " '@editorahumanas',\n",
       " 'mídia',\n",
       " 'primeiro',\n",
       " 'usar',\n",
       " 'essas',\n",
       " 'Isso',\n",
       " 'será',\n",
       " 'países',\n",
       " 'seria',\n",
       " 'apenas',\n",
       " 'saber',\n",
       " 'feminismo',\n",
       " 'favor',\n",
       " 'esses',\n",
       " 'Quem',\n",
       " 'negros',\n",
       " 'filho',\n",
       " 'feministas',\n",
       " '@JOAQUINVOLTOU',\n",
       " 'parece',\n",
       " 'tanto',\n",
       " 'existe',\n",
       " 'ficar',\n",
       " 'Mulher',\n",
       " 'ninguém',\n",
       " 'tinha',\n",
       " 'puta',\n",
       " 'maior',\n",
       " 'fora',\n",
       " 'sim,',\n",
       " 'sair',\n",
       " 'todas',\n",
       " 'dinheiro',\n",
       " '@LeoLimaDuarte',\n",
       " 'dessa',\n",
       " 'feliz',\n",
       " 'alguém',\n",
       " 'culpa',\n",
       " 'noite',\n",
       " 'querem',\n",
       " 'outro',\n",
       " 'tava',\n",
       " 'qualquer',\n",
       " 'deve',\n",
       " 'dela',\n",
       " 'Bolsonaro',\n",
       " 'machista',\n",
       " 'Parabéns',\n",
       " 'dizem',\n",
       " 'mesma',\n",
       " 'novo',\n",
       " 'pessoa',\n",
       " 'este',\n",
       " '@PastorMalafaia',\n",
       " 'mulher,',\n",
       " 'posso',\n",
       " 'vamos',\n",
       " 'Mais',\n",
       " 'aceita',\n",
       " 'verdade',\n",
       " 'povo',\n",
       " 'direito',\n",
       " 'sabem',\n",
       " 'volta',\n",
       " 'pena',\n",
       " 'imprensa',\n",
       " 'vocês',\n",
       " 'foto',\n",
       " 'bandido',\n",
       " 'porra',\n",
       " 'umas',\n",
       " 'sendo',\n",
       " '@MarleneBicalho',\n",
       " 'crianças',\n",
       " 'Agora',\n",
       " 'orgulho',\n",
       " 'galera',\n",
       " 'amiga',\n",
       " 'Orgulho',\n",
       " 'algo',\n",
       " 'acabar',\n",
       " 'queria',\n",
       " 'isto',\n",
       " 'Europa',\n",
       " 'sido',\n",
       " 'brasil',\n",
       " 'ganhar',\n",
       " 'lado',\n",
       " 'então',\n",
       " 'Esse',\n",
       " 'família',\n",
       " 'fazendo',\n",
       " '@jairbolsonaro',\n",
       " 'meio',\n",
       " 'dele',\n",
       " 'amigo',\n",
       " 'deus',\n",
       " 'dilma',\n",
       " 'claro',\n",
       " 'igual',\n",
       " 'estou',\n",
       " 'tirar',\n",
       " 'HOMEM',\n",
       " 'ontem',\n",
       " 'vale',\n",
       " 'piada',\n",
       " 'suas',\n",
       " 'mudar',\n",
       " 'realmente',\n",
       " 'amigos',\n",
       " 'pegar',\n",
       " 'caso',\n",
       " '#PAZ',\n",
       " 'problema',\n",
       " 'seja',\n",
       " 'perfil',\n",
       " 'comunicação',\n",
       " 'gosto',\n",
       " 'mina',\n",
       " 'conhece',\n",
       " 'temer',\n",
       " 'branca',\n",
       " 'Para',\n",
       " 'quanto',\n",
       " 'Vamos',\n",
       " 'linda',\n",
       " 'aquela',\n",
       " 'gays',\n",
       " 'ideologia',\n",
       " 'Então',\n",
       " 'Brasil',\n",
       " 'nessa',\n",
       " 'coisas',\n",
       " 'Presidente',\n",
       " 'somos',\n",
       " 'racista',\n",
       " 'receber',\n",
       " 'Acho',\n",
       " '@amora_inlove',\n",
       " 'mimimi',\n",
       " '@gabrisinhas',\n",
       " 'causa',\n",
       " 'Primeiro',\n",
       " 'elas',\n",
       " 'olha',\n",
       " 'governo',\n",
       " 'numa',\n",
       " 'achando',\n",
       " '@realDonaldTrump',\n",
       " 'namorada',\n",
       " 'bonita',\n",
       " 'semana',\n",
       " 'nenhuma',\n",
       " 'enquanto',\n",
       " 'chegar',\n",
       " 'deixar',\n",
       " 'politicamente',\n",
       " 'amigas',\n",
       " 'viado',\n",
       " 'discurso',\n",
       " 'bunda',\n",
       " '@agdabritto',\n",
       " 'morre',\n",
       " 'filhos',\n",
       " 'outras',\n",
       " 'outros',\n",
       " 'Islão',\n",
       " 'falta',\n",
       " 'forma',\n",
       " 'pois',\n",
       " 'demais',\n",
       " 'MINHA',\n",
       " 'cabelo',\n",
       " 'fico',\n",
       " 'acabou',\n",
       " 'cabeça',\n",
       " 'humor',\n",
       " 'futebol',\n",
       " 'filha',\n",
       " 'odeio',\n",
       " 'fufas,',\n",
       " 'pensa',\n",
       " 'burra,',\n",
       " 'vagabundo',\n",
       " 'ISSO',\n",
       " 'Essa',\n",
       " 'pras',\n",
       " 'vezes',\n",
       " 'dormir',\n",
       " 'precisa',\n",
       " 'trabalho',\n",
       " 'voltar',\n",
       " 'sentindo',\n",
       " 'dias',\n",
       " 'cota',\n",
       " 'gênero',\n",
       " 'amor',\n",
       " 'teve',\n",
       " 'Bilhetes',\n",
       " 'merda,',\n",
       " '@angelicamorango',\n",
       " 'nome',\n",
       " 'menina',\n",
       " 'MAIS',\n",
       " '@Justiceiro_Sujo',\n",
       " 'CARA',\n",
       " 'fotos',\n",
       " 'poder',\n",
       " 'tanta',\n",
       " 'crime',\n",
       " 'acaba',\n",
       " 'frente',\n",
       " 'aquele',\n",
       " 'Porto',\n",
       " 'Aqui',\n",
       " 'deveria',\n",
       " 'temos',\n",
       " 'manda',\n",
       " 'opinião',\n",
       " 'preconceito',\n",
       " 'pior',\n",
       " 'ficam',\n",
       " 'desde',\n",
       " 'VERGONHA',\n",
       " 'GORDA',\n",
       " 'valor',\n",
       " '@melaineribeiro',\n",
       " '@HumorNegroo',\n",
       " 'horas',\n",
       " '@amandassini',\n",
       " '2017',\n",
       " 'consegue',\n",
       " 'mulher.',\n",
       " 'terá',\n",
       " 'social',\n",
       " 'vergonha',\n",
       " 'começa',\n",
       " 'sexo',\n",
       " 'parar',\n",
       " 'ódio',\n",
       " 'olhos',\n",
       " 'país.',\n",
       " 'massa',\n",
       " 'minhas',\n",
       " 'começar',\n",
       " 'ESSE',\n",
       " 'história',\n",
       " 'problemas',\n",
       " 'NADA',\n",
       " 'pros',\n",
       " 'Benfica',\n",
       " '@TonhoDrinks',\n",
       " 'Qual',\n",
       " 'terrorista',\n",
       " 'turma',\n",
       " 'segurança',\n",
       " 'trabalhar',\n",
       " 'presidente',\n",
       " 'Grande',\n",
       " 'hoje,',\n",
       " 'nossos',\n",
       " 'fazem',\n",
       " 'FEIA',\n",
       " '@gentebranca1',\n",
       " '#orgulhohetero',\n",
       " 'paga',\n",
       " 'roupa',\n",
       " '@karlariane',\n",
       " 'luta',\n",
       " '@fcancio',\n",
       " 'vontade',\n",
       " 'não!',\n",
       " 'milhões',\n",
       " 'Porque',\n",
       " 'maioria',\n",
       " 'mas,',\n",
       " 'pretos',\n",
       " 'muçulmanos',\n",
       " 'Maria',\n",
       " 'qual',\n",
       " 'nenhum',\n",
       " 'MUITO',\n",
       " 'dizendo',\n",
       " 'medo',\n",
       " 'cultura',\n",
       " 'chamou',\n",
       " 'correto',\n",
       " 'comer',\n",
       " 'Você',\n",
       " 'primeira',\n",
       " 'conta',\n",
       " 'mulher\"',\n",
       " 'Isto',\n",
       " 'Feliz',\n",
       " 'acreditar',\n",
       " 'Brasil.',\n",
       " 'lixo',\n",
       " 'ouvir',\n",
       " 'pele',\n",
       " 'estes',\n",
       " 'Ministro',\n",
       " 'mesmo,',\n",
       " 'twitter',\n",
       " 'foram',\n",
       " 'comigo',\n",
       " 'preciso',\n",
       " 'diferença',\n",
       " 'devem',\n",
       " 'ESSA',\n",
       " 'absurdo',\n",
       " 'GENTE',\n",
       " 'bando',\n",
       " 'ditadura',\n",
       " 'passou',\n",
       " 'mandar',\n",
       " 'mesmo.',\n",
       " 'gordo',\n",
       " 'MERDA',\n",
       " 'mulheres.',\n",
       " 'Tenho',\n",
       " 'trump',\n",
       " '@vitormramalho',\n",
       " 'sente',\n",
       " 'pergunta',\n",
       " 'comprar',\n",
       " '#Trump',\n",
       " 'GRANDE',\n",
       " 'quiser',\n",
       " 'miga',\n",
       " 'sentir',\n",
       " 'vira',\n",
       " 'Angola',\n",
       " '#JOAQUINRESPONDE',\n",
       " 'hein',\n",
       " '#Islão',\n",
       " 'dia,',\n",
       " 'www.aescritoraeomusico.blogspot.com',\n",
       " 'estaria',\n",
       " 'jogadores',\n",
       " 'inverso',\n",
       " 'feia.',\n",
       " 'Oriente',\n",
       " 'mães',\n",
       " 'come',\n",
       " 'sinto',\n",
       " 'apoio',\n",
       " 'continua',\n",
       " 'moral',\n",
       " 'pensar',\n",
       " 'chamando',\n",
       " 'criança',\n",
       " 'além',\n",
       " 'dentro',\n",
       " 'continuar',\n",
       " 'carne',\n",
       " 'programa',\n",
       " 'algum',\n",
       " 'estamos',\n",
       " 'disso',\n",
       " 'única',\n",
       " 'brancos',\n",
       " 'mostra',\n",
       " 'Obama',\n",
       " 'chega',\n",
       " 'Twitter',\n",
       " 'censura',\n",
       " 'nesse',\n",
       " 'dois',\n",
       " 'Pode',\n",
       " 'resto',\n",
       " 'Ainda',\n",
       " 'respeito',\n",
       " 'pega',\n",
       " 'Globo',\n",
       " 'deveriam',\n",
       " 'parabéns',\n",
       " 'foda',\n",
       " 'inteiro',\n",
       " 'tiver',\n",
       " 'passo',\n",
       " 'seguir',\n",
       " 'Mundo',\n",
       " 'gente,',\n",
       " 'rede',\n",
       " 'juiz',\n",
       " 'usam',\n",
       " 'bate',\n",
       " 'vitória',\n",
       " 'adoro',\n",
       " 'andar',\n",
       " 'sério',\n",
       " 'nível',\n",
       " 'acham',\n",
       " 'quase',\n",
       " 'devia',\n",
       " 'ruim',\n",
       " 'PARA',\n",
       " 'certeza',\n",
       " 'levar',\n",
       " '@GABRIELPlNHEIRO',\n",
       " 'GOSTO',\n",
       " 'Estamos',\n",
       " 'namorar',\n",
       " 'livre',\n",
       " 'criminosos',\n",
       " 'JUSTIÇA',\n",
       " 'difícil',\n",
       " 'casa,',\n",
       " 'Força',\n",
       " 'jeito',\n",
       " 'banheiro',\n",
       " 'caralho',\n",
       " 'estava',\n",
       " 'tarde',\n",
       " 'pessoal',\n",
       " 'palavra',\n",
       " 'atenção',\n",
       " 'NOTÍCIAS',\n",
       " 'falou',\n",
       " 'emprego',\n",
       " 'cunha',\n",
       " 'manhã',\n",
       " 'AGORA',\n",
       " 'nordeste',\n",
       " 'peitos',\n",
       " '\"essa',\n",
       " 'falam',\n",
       " 'Hetero',\n",
       " 'vermelho',\n",
       " 'TIRAR',\n",
       " 'adao',\n",
       " 'letras',\n",
       " 'assistir',\n",
       " 'vender',\n",
       " 'capitalismo',\n",
       " 'VOCÊ',\n",
       " 'PROBLEMA',\n",
       " 'criou',\n",
       " 'REDE',\n",
       " 'mostrar',\n",
       " 'muita',\n",
       " 'cidade',\n",
       " 'João',\n",
       " 'querer',\n",
       " 'feia\"',\n",
       " 'ajudar',\n",
       " 'estado',\n",
       " 'duas',\n",
       " 'passa',\n",
       " 'casal',\n",
       " 'macho',\n",
       " 'demasiado',\n",
       " 'movimento',\n",
       " 'Olha',\n",
       " 'podia',\n",
       " 'deixa',\n",
       " 'sentido',\n",
       " 'achei',\n",
       " 'pede',\n",
       " 'oportunidade',\n",
       " '\"Overdose',\n",
       " '@maathbz',\n",
       " '@YouTube',\n",
       " 'durante',\n",
       " 'daqui',\n",
       " 'política',\n",
       " 'sabia',\n",
       " 'imagem',\n",
       " 'monte',\n",
       " 'meses',\n",
       " 'veja',\n",
       " 'acordo',\n",
       " 'portugueses',\n",
       " 'cedo',\n",
       " 'AINDA',\n",
       " 'Nunca',\n",
       " 'Nacional',\n",
       " 'NUNCA',\n",
       " 'Facebook',\n",
       " 'lembra',\n",
       " 'morreu',\n",
       " 'alguma',\n",
       " 'feito',\n",
       " 'vários',\n",
       " 'socialismo',\n",
       " 'PODE',\n",
       " 'jornalismo',\n",
       " 'meninas',\n",
       " 'problema.',\n",
       " 'alguns',\n",
       " 'ficou',\n",
       " 'construção',\n",
       " 'maconha',\n",
       " 'dando',\n",
       " 'linda,',\n",
       " 'Espero',\n",
       " 'mulher!',\n",
       " 'político',\n",
       " 'TUDO',\n",
       " 'mostrando',\n",
       " 'reclama',\n",
       " 'pobre',\n",
       " 'merece',\n",
       " 'tomar',\n",
       " 'desculpa',\n",
       " 'APROPRIAÇÃO',\n",
       " 'metade',\n",
       " 'feminino',\n",
       " 'Temos',\n",
       " 'anda',\n",
       " 'agora?',\n",
       " 'apoia',\n",
       " 'Aliás,',\n",
       " 'responde',\n",
       " 'atrás',\n",
       " 'Mas,',\n",
       " 'direita',\n",
       " 'rsrs',\n",
       " 'tradicional',\n",
       " 'mentira',\n",
       " 'terroristas',\n",
       " 'dúvida',\n",
       " 'veio',\n",
       " 'página',\n",
       " 'Minha',\n",
       " 'aqui,',\n",
       " 'Hoje',\n",
       " 'prefiro',\n",
       " '@antoniocostapm',\n",
       " 'situação',\n",
       " 'avião',\n",
       " 'prefere',\n",
       " 'sala',\n",
       " 'classe',\n",
       " 'SABE',\n",
       " 'deputado',\n",
       " 'não.',\n",
       " 'FALSAS',\n",
       " 'Lula',\n",
       " 'burro',\n",
       " 'Deus',\n",
       " 'pedir',\n",
       " 'Imprensa',\n",
       " 'nada.',\n",
       " 'grupo',\n",
       " 'empregada',\n",
       " 'entra',\n",
       " 'FEMINISTA',\n",
       " 'Estado',\n",
       " 'desejo',\n",
       " 'social.',\n",
       " 'manter',\n",
       " 'junto',\n",
       " 'vida.',\n",
       " 'preço',\n",
       " 'lugar.',\n",
       " 'espere',\n",
       " 'sapatão,',\n",
       " 'TENHO',\n",
       " 'liga',\n",
       " 'Tudo',\n",
       " '@aninha_ebc',\n",
       " 'terra',\n",
       " 'burra.',\n",
       " 'série',\n",
       " 'muitos',\n",
       " 'logo',\n",
       " 'Burra',\n",
       " 'Será',\n",
       " 'violência',\n",
       " 'corpo',\n",
       " 'achar',\n",
       " '@BolsonaroSP',\n",
       " 'desses',\n",
       " 'campanha',\n",
       " 'desse',\n",
       " 'liberdade',\n",
       " 'tivesse',\n",
       " 'brasileira',\n",
       " 'eleições',\n",
       " 'trans',\n",
       " '@RuiHCruz',\n",
       " 'BRASIL',\n",
       " 'Mulheres',\n",
       " 'querendo',\n",
       " 'entrevista',\n",
       " 'esperar',\n",
       " 'país,',\n",
       " 'anos,',\n",
       " 'Quer',\n",
       " '@marisa_lobo',\n",
       " 'moda',\n",
       " 'jantar',\n",
       " 'Obrigado',\n",
       " 'deles',\n",
       " 'pouco',\n",
       " 'parte',\n",
       " 'boca',\n",
       " 'meus',\n",
       " 'existem',\n",
       " 'refugiados,',\n",
       " 'tenha',\n",
       " 'algumas',\n",
       " 'bolsonaro',\n",
       " 'feminazi',\n",
       " '\"refugiados\"',\n",
       " 'Eles',\n",
       " 'real',\n",
       " 'aceitar',\n",
       " 'EUA.',\n",
       " 'processo',\n",
       " 'amanhã',\n",
       " 'concorda',\n",
       " 'sonho',\n",
       " 'saiu',\n",
       " 'Islão,',\n",
       " 'escrever',\n",
       " 'grávida',\n",
       " 'quantidade',\n",
       " 'verdade.',\n",
       " 'candidato',\n",
       " 'defende',\n",
       " 'fiquei',\n",
       " 'também.',\n",
       " 'video',\n",
       " 'CHEGA',\n",
       " 'vice',\n",
       " 'burca',\n",
       " 'esquerdista',\n",
       " 'você,',\n",
       " 'prova',\n",
       " 'gostar',\n",
       " 'escola',\n",
       " 'Conselho',\n",
       " 'comida',\n",
       " 'ciência',\n",
       " 'ganhou',\n",
       " 'internet',\n",
       " 'nascer',\n",
       " 'entendi',\n",
       " 'Ideologia',\n",
       " 'serve',\n",
       " 'piadinhas',\n",
       " 'diria',\n",
       " 'Liga',\n",
       " 'chineses',\n",
       " 'FEMINISTAS',\n",
       " 'tentando',\n",
       " 'anos.',\n",
       " 'passam',\n",
       " 'machistas',\n",
       " 'Melhor',\n",
       " 'bilhetes',\n",
       " 'obrigada',\n",
       " 'decote',\n",
       " 'pensei',\n",
       " 'entram',\n",
       " 'completamente',\n",
       " 'machismo',\n",
       " 'agora,',\n",
       " 'falo',\n",
       " 'tenta',\n",
       " 'Santa',\n",
       " 'falei',\n",
       " 'comunismo',\n",
       " 'presente',\n",
       " 'assume',\n",
       " 'viajar',\n",
       " 'imagina',\n",
       " 'Deus,',\n",
       " 'tantos',\n",
       " 'chorar',\n",
       " 'homofobia',\n",
       " 'voto',\n",
       " 'familia',\n",
       " 'deixou',\n",
       " 'lavar',\n",
       " 'Estão',\n",
       " 'Antes',\n",
       " '@pnr',\n",
       " 'fizeram',\n",
       " 'inferno',\n",
       " 'sei,',\n",
       " 'PHOTO',\n",
       " 'HOJE',\n",
       " 'hetero',\n",
       " 'melhores',\n",
       " 'capital',\n",
       " 'Seja',\n",
       " 'Fica',\n",
       " 'mais!',\n",
       " 'odeiam',\n",
       " '@pensaporti',\n",
       " 'reclamar',\n",
       " 'gastar',\n",
       " 'gostosas',\n",
       " 'Este',\n",
       " 'serem',\n",
       " 'saia',\n",
       " 'preto',\n",
       " 'QUANDO',\n",
       " 'fuçada',\n",
       " 'jogo',\n",
       " 'mariquice',\n",
       " 'negras',\n",
       " 'conheço',\n",
       " 'homicídio',\n",
       " 'escolas',\n",
       " 'hahaha',\n",
       " 'reviso)',\n",
       " 'beber',\n",
       " 'frase',\n",
       " 'mesquita',\n",
       " 'três',\n",
       " 'hein?',\n",
       " 'sede',\n",
       " 'vindo',\n",
       " 'notícias',\n",
       " 'dia!',\n",
       " '@Quase_Gay',\n",
       " 'candidatura',\n",
       " 'Natal',\n",
       " 'prints',\n",
       " 'esteve',\n",
       " 'Infelizmente',\n",
       " '@caiquepala',\n",
       " 'teresa',\n",
       " 'jovens',\n",
       " 'live',\n",
       " '@_jujuba_lima',\n",
       " 'Vocês',\n",
       " 'pelos',\n",
       " 'mortos',\n",
       " 'pagar',\n",
       " '@alex_borges',\n",
       " 'colocar',\n",
       " 'vídeo',\n",
       " 'Costa',\n",
       " 'nova',\n",
       " 'estupro',\n",
       " 'após',\n",
       " 'aguento',\n",
       " 'FELIZ',\n",
       " '@CapitalismoOP',\n",
       " 'menino',\n",
       " 'novos',\n",
       " 'pediu',\n",
       " 'kkkkk',\n",
       " '#Oscars',\n",
       " 'Sporting',\n",
       " 'idiota',\n",
       " 'BURRA',\n",
       " 'magra,',\n",
       " 'sejam',\n",
       " 'mata',\n",
       " 'bonita,',\n",
       " 'sociedade',\n",
       " 'marido',\n",
       " 'democratas',\n",
       " 'MULHERES',\n",
       " 'faço',\n",
       " 'imigração',\n",
       " 'saudades',\n",
       " '@OdeCarvalho',\n",
       " 'Nada',\n",
       " 'kkkkkk',\n",
       " 'Estou',\n",
       " 'puta,',\n",
       " 'que,',\n",
       " 'identidade',\n",
       " 'direitos',\n",
       " '2016',\n",
       " 'virou',\n",
       " 'provar',\n",
       " 'FAZENDO',\n",
       " 'últimos',\n",
       " 'chata',\n",
       " 'argumento',\n",
       " 'piranha',\n",
       " 'perder',\n",
       " 'pontos',\n",
       " 'falido',\n",
       " 'conseguem',\n",
       " 'nacional',\n",
       " 'conversa',\n",
       " 'manifestação',\n",
       " 'coloca',\n",
       " 'tive',\n",
       " '@FlavioBolsonaro',\n",
       " 'preso',\n",
       " 'tipo',\n",
       " 'balada',\n",
       " 'morte',\n",
       " 'queremos',\n",
       " '@SpOAB',\n",
       " 'velha',\n",
       " 'ameaça',\n",
       " 'Feminismo',\n",
       " 'Bela',\n",
       " 'acredito',\n",
       " 'corrupção',\n",
       " 'blog',\n",
       " 'pública',\n",
       " 'Ontem',\n",
       " 'cobra',\n",
       " 'obra',\n",
       " 'próprio',\n",
       " 'certo,',\n",
       " 'greve',\n",
       " 'indo',\n",
       " 'reunião',\n",
       " 'gay,',\n",
       " 'acontece',\n",
       " 'começando',\n",
       " 'morrer',\n",
       " 'Pela',\n",
       " 'quis',\n",
       " 'crise',\n",
       " 'ANOS',\n",
       " 'vossa',\n",
       " 'sofre',\n",
       " 'legal',\n",
       " 'motivo',\n",
       " 'passando',\n",
       " 'partir',\n",
       " 'Esta',\n",
       " 'Assim',\n",
       " 'pais',\n",
       " 'militar',\n",
       " 'jogar',\n",
       " '@nytimes',\n",
       " 'finalmente',\n",
       " 'tentar',\n",
       " 'pelas',\n",
       " 'cristãos',\n",
       " 'Braga',\n",
       " 'lidar',\n",
       " 'representa',\n",
       " 'esquerda.',\n",
       " 'quiser.',\n",
       " 'Desde',\n",
       " 'musica',\n",
       " 'DEIXA',\n",
       " 'mais.',\n",
       " 'feminista?',\n",
       " 'gostam',\n",
       " 'saudade',\n",
       " 'seguidoras',\n",
       " 'olho',\n",
       " 'Fufas',\n",
       " 'erro',\n",
       " 'vagabunda',\n",
       " 'PNR,',\n",
       " 'feira',\n",
       " 'virar',\n",
       " 'CARNAVAL',\n",
       " 'torna',\n",
       " 'perdeu',\n",
       " 'imigrantes',\n",
       " 'doente',\n",
       " 'entrou',\n",
       " 'negra',\n",
       " 'bosta',\n",
       " 'inclusive',\n",
       " 'Depois',\n",
       " 'sofrendo',\n",
       " 'fome',\n",
       " 'rola',\n",
       " 'salário',\n",
       " 'comunista',\n",
       " 'solução',\n",
       " 'PAREM',\n",
       " 'roubar',\n",
       " 'significado',\n",
       " 'apesar',\n",
       " 'cidadãos',\n",
       " 'facebook',\n",
       " 'morta',\n",
       " 'contrário,',\n",
       " 'Somos',\n",
       " ...]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_4words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17338baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d3b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
